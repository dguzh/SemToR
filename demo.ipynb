{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd08b16d-2fb5-4e24-8153-234b8bf32036",
   "metadata": {},
   "source": [
    "# Fine-Tuning SentenceTransformers for Toponym Resolution\n",
    "\n",
    "This notebook outlines the process of adapting SentenceTransformer models for toponym resolution.\n",
    "\n",
    "Before proceeding, ensure that all necessary packages are installed by running `pip install -r requirements.txt` to install the dependencies listed in the requirements file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68c614-ea50-4d36-a8e3-693abca7f6f5",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports\n",
    "\n",
    "First, we import necessary modules from our `src` package. These modules are categorized as follows:\n",
    "\n",
    "- `toponyms.py` for handling the toponym datasets\n",
    "- `gazetteer.py` for working with the gazetteer data\n",
    "- `indexing.py` for indexing and candidate generation\n",
    "- `training.py` for model training utilities\n",
    "- `evaluation.py` for model prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1c7e9-fc5c-47ef-ab03-53aad2a1bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.toponyms import *\n",
    "from src.gazetteer import *\n",
    "from src.indexing import *\n",
    "from src.training import *\n",
    "from src.evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4c8de-f299-4129-b108-5736ef72b7be",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "This notebook relies on various data sources, including toponym datasets and gazetteer data from GeoNames. Please follow the links provided below to download the necessary files. After downloading, organise and name the files according to the specified folder structure to ensure the code runs without issues.\n",
    "\n",
    "### Download Links\n",
    "\n",
    "- Toponym Datasets:\n",
    "    - [Download `lgl.xml`](https://github.com/milangritta/Pragmatic-Guide-to-Geoparsing-Evaluation/blob/master/data/Corpora/lgl.xml)\n",
    "    - [Download `gwn.xml`](https://github.com/milangritta/Pragmatic-Guide-to-Geoparsing-Evaluation/blob/master/data/GWN.xml)\n",
    "    - [Download `trn.xml`](https://github.com/milangritta/Pragmatic-Guide-to-Geoparsing-Evaluation/blob/master/data/Corpora/TR-News.xml)\n",
    "- Gazetteer Data:\n",
    "    - [Download `allCountries.txt`](https://download.geonames.org/export/dump/allCountries.zip)\n",
    "    - [Download `admin1CodesASCII.txt`](https://download.geonames.org/export/dump/admin1CodesASCII.txt)\n",
    "    - [Download `admin2Codes.txt`](https://download.geonames.org/export/dump/admin2Codes.txt)\n",
    "    - [Download `countryInfo.txt`](https://download.geonames.org/export/dump/countryInfo.txt)\n",
    "    - [Download `featureCodes_en.txt`](https://download.geonames.org/export/dump/featureCodes_en.txt)\n",
    "- Demonyms:\n",
    "    - [Download `demonyms.csv`](https://github.com/knowitall/chunkedextractor/blob/master/src/main/resources/edu/knowitall/chunkedextractor/demonyms.csv)\n",
    "\n",
    "### Folder Structure\n",
    "\n",
    "Ensure that the downloaded files are correctly named and saved in the following structure within the project directory:\n",
    "\n",
    "```\n",
    "SemToR/\n",
    "│\n",
    "└───data/\n",
    "    │\n",
    "    ├───texts/\n",
    "    │   ├───lgl.xml\n",
    "    │   ├───gwn.xml\n",
    "    │   └───trn.xml\n",
    "    │\n",
    "    ├───geonames/\n",
    "    │   ├───allCountries.txt\n",
    "    │   ├───admin1CodesASCII.txt\n",
    "    │   ├───admin2Codes.txt\n",
    "    │   ├───countryInfo.txt\n",
    "    │   └───featureCodes_en.txt\n",
    "    │\n",
    "    └───demonyms.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdcfb48-1867-4ea5-ab14-8efeb3a31310",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGL_PATH = 'data/texts/lgl.xml'\n",
    "GWN_PATH = 'data/texts/gwn.xml'\n",
    "TRN_PATH = 'data/texts/trn.xml'\n",
    "GAZETTEER_PATH = 'data/geonames/allCountries.txt'\n",
    "ADMIN1_PATH = 'data/geonames/admin1CodesASCII.txt'\n",
    "ADMIN2_PATH = 'data/geonames/admin2Codes.txt'\n",
    "COUNTRY_PATH = 'data/geonames/countryInfo.txt'\n",
    "FEATURE_PATH = 'data/geonames/featureCodes_en.txt'\n",
    "DEMONYMS_PATH = 'data/demonyms.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c72f4-071d-4a98-ac66-127cef43cecb",
   "metadata": {},
   "source": [
    "---\n",
    "## Toponym Index\n",
    "\n",
    "We construct an index that maps toponyms to their potential location candidates using data from the GeoNames gazetteer. This index is a Python dictionary where keys are normalized toponym strings and values are lists of GeoName IDs. We extend the index with demonyms to cover more potential string matches.\n",
    "\n",
    "We start by loading the gazetteer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e8f3e-5a4f-421a-8147-59cb3860fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazetteer_df = load_gazetteer(GAZETTEER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ad62d-42ab-4fe6-943b-4f66e5399fde",
   "metadata": {},
   "source": [
    "We then generate the toponym index from the gazetteer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4555d-041d-4378-8f80-4da6b04f93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "toponym_index = build_toponym_index(gazetteer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267e224-296b-4dde-950c-9026cfa9bd90",
   "metadata": {},
   "source": [
    "Finally, we extend the index with the singular and plural demonymic forms of 2144 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802beb5-ddc6-414b-830b-efb4a4a7234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toponym_index = extend_index_with_demonyms(toponym_index, DEMONYMS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1b8b8-cd2f-468b-9537-d14a1162a8d9",
   "metadata": {},
   "source": [
    "---\n",
    "## Toponym Datasets\n",
    "\n",
    "To train and evaluate our models for toponym resolution, we prepare three toponym datasets of English news articles:\n",
    "- [Local Global Lexicon (Lieberman et al. 2010)](https://doi.org/10.1109/ICDE.2010.5447903)\n",
    "- [GeoWebNews (Gritta et al. 2020)](https://doi.org/10.1007/s10579-019-09475-3)\n",
    "- [TR-News (Kamalloo and Rafiei 2018)](https://doi.org/10.1145/3178876.3186027)\n",
    "\n",
    "We start by loading the datasets, which contain news article texts along with annotated toponyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183ff25-c6c4-40cc-9d1e-b0d7e335af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgl_df = load_toponyms(LGL_PATH)\n",
    "gwn_df = load_toponyms(GWN_PATH)\n",
    "trn_df = load_toponyms(TRN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bca12a-c5c0-491f-bdb5-18c14d947794",
   "metadata": {},
   "source": [
    "Next, we filter out toponyms with invalid GeoName IDs by removing rows for which the GeoName IDs are not present in the gazetteer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38cf56-9fd4-41b3-98b8-83acf1ad4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgl_df = filter_toponyms(lgl_df, gazetteer_df)\n",
    "gwn_df = filter_toponyms(gwn_df, gazetteer_df)\n",
    "trn_df = filter_toponyms(trn_df, gazetteer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285ace4-0cab-4618-aa42-762e55063244",
   "metadata": {},
   "source": [
    "For each toponym, we generate a list of candidate locations using our previously created index. This process might require querying the GeoNames API for any toponym string that isn't covered by the index.\n",
    "\n",
    "> **Important Note on API Calls and Caching**: All responses from the GeoNames API have been cached in the provided `geonames_cache.pkl` file. This cache ensures that no new API calls are required to process the datasets used in this project, reducing the need for individual users to use their own GeoNames accounts. However, if datasets are updated or extended beyond the scope of the cached responses, users may need to make new API calls, which would require a personal GeoNames username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665dcbf-1312-4474-9925-9c359d9a282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEONAMES_USERNAME = 'demo'\n",
    "\n",
    "lgl_df = generate_candidates(lgl_df, toponym_index, username=GEONAMES_USERNAME)\n",
    "gwn_df = generate_candidates(gwn_df, toponym_index, username=GEONAMES_USERNAME)\n",
    "trn_df = generate_candidates(trn_df, toponym_index, username=GEONAMES_USERNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f4671-1738-4d79-9b92-8b489da56762",
   "metadata": {},
   "source": [
    "Finally, we truncate the texts to meet the model's maximum sequence length requirements. At this stage, we specify the model that will be fine-tuned later, ensuring that the texts are appropriately prepared for that specific model's input limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171f001-ac0a-4dbd-b6b3-fc1bbfd8585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "\n",
    "MODEL_LIMITS = {\n",
    "    'all-MiniLM-L6-v2': 256,\n",
    "    'all-MiniLM-L12-v2': 256,\n",
    "    'all-distilroberta-v1': 512,\n",
    "    'all-mpnet-base-v2': 384,\n",
    "    'multi-qa-MiniLM-L6-cos-v1': 512,\n",
    "    'multi-qa-distilbert-cos-v1': 512,\n",
    "    'multi-qa-mpnet-base-dot-v1': 512\n",
    "}\n",
    "\n",
    "lgl_df = truncate_texts(lgl_df, f'sentence-transformers/{MODEL_NAME}', MODEL_LIMITS[MODEL_NAME])\n",
    "gwn_df = truncate_texts(gwn_df, f'sentence-transformers/{MODEL_NAME}', MODEL_LIMITS[MODEL_NAME])\n",
    "trn_df = truncate_texts(trn_df, f'sentence-transformers/{MODEL_NAME}', MODEL_LIMITS[MODEL_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787427c2-8f85-471b-acda-3d175a8cc55a",
   "metadata": {},
   "source": [
    "---\n",
    "## Location Candidates\n",
    "\n",
    "For the models to be able to generate embeddings for location candidates, we create textual representations of gazetteer entries.\n",
    "\n",
    "First, we reduce the size of our gazetteer to include only the entries that are actual candidates for the toponyms in our datasets. This step helps in minimizing the computational resources needed for subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c2046-3912-441e-9697-b7082b897089",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazetteer_df = filter_gazetteer(gazetteer_df, [lgl_df, gwn_df, trn_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc99d5-6413-46ae-ac4e-7a596ff83028",
   "metadata": {},
   "source": [
    "The GeoNames gazetteer uses codes to represent countries, administrative divisions, and feature types. We map these codes to their corresponding names using the downloaded lookup tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90209d-61f3-479c-a3c0-24f9944f6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazetteer_df = generate_descriptor_names(gazetteer_df, ADMIN1_PATH, ADMIN2_PATH, COUNTRY_PATH, FEATURE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bc09a6-f8e1-4e34-b9b0-6ca81aa34568",
   "metadata": {},
   "source": [
    "With the names extracted, we then generate pseudotexts for each gazetteer entry. These pseudotexts follow the format:\n",
    "\n",
    "`[name] ([feature type]) in [admin2], [admin1], [country]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ce23a-5042-4dd0-95e4-e68f350d05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazetteer_df = generate_pseudotexts(gazetteer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221472a-da8b-4f5a-ad51-062de03cd8d6",
   "metadata": {},
   "source": [
    "---\n",
    "## Fine-Tuning\n",
    "\n",
    "This section outlines the process of fine-tuning the SentenceTransformer model using our prepared datasets.\n",
    "\n",
    "First, we import the necessary components from the SentenceTransformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4f703-ab2f-4a62-be8a-709cdc70cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03598953-17a8-4b9c-bcef-3d2f684c219b",
   "metadata": {},
   "source": [
    "The datasets are divided into training (70%), evaluation (10%), and test sets (20%). While the training and evaluation sets are pooled from all datasets, the test sets are kept separate for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02188b19-03ed-4348-86c5-021406e21bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df, (test_lgl_df, test_gwn_df, test_trn_df) = split_train_eval_test(lgl_df, gwn_df, trn_df, test_size=0.2, eval_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9014d-4cb4-40d0-aa57-6680543d973d",
   "metadata": {},
   "source": [
    "We define the batch size based on our computational resources. Using a NVIDIA GeForce RTX 3070, we found a batch size of 8 to be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84191ed-40b6-4656-89b9-eaac7099c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c4442-7e18-40b7-9c4b-7a04c6678ba1",
   "metadata": {},
   "source": [
    "Dataloader and evaluator objects are then created for use during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8621e2e-099c-4661-8e6a-29a4a8c3b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader(train_df, gazetteer_df, BATCH_SIZE)\n",
    "evaluator = ToponymResolutionEvaluator(gazetteer_df, eval_df, model_name=MODEL_NAME, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a286b-86e5-4b9f-86e7-b554a3467aa0",
   "metadata": {},
   "source": [
    "The SentenceTransformer model is instantiated with the previously specified model name, which identifies the pre-trained model to be fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9c853-848a-4247-be60-cfd3fe816d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d10b3-c3ce-420d-bfbd-6ba0fed5dad6",
   "metadata": {},
   "source": [
    "We use a contrastive loss function which is suitable for the task of learning to distinguish between similar and dissimilar pairs of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e74789-dc4b-43cd-80d4-ff4407538a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.ContrastiveLoss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d3b9c-a34a-4596-9c9f-39b788f9cdff",
   "metadata": {},
   "source": [
    "Finally, we fine-tune the model. Hyperparameters have been specified following the code examples in the [documentation](https://www.sbert.net/docs/training/overview.html).  No hyperparameter optimisation was performed.\n",
    "\n",
    "During training, checkpoints are saved in the `model_checkpoints` directory, and evaluation results are stored as CSV files in `evaluation_results`. The best-performing model, based on accuracy on the evaluation set, is saved in the `models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08a781-8ea7-4ef1-bd6e-a25730596c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_objectives=[(dataloader, loss)],\n",
    "          epochs=1,\n",
    "          warmup_steps=100,\n",
    "          evaluator=evaluator,\n",
    "          evaluation_steps=len(dataloader) // 10,\n",
    "          checkpoint_save_steps=len(dataloader) // 10,\n",
    "          checkpoint_path=f'model_checkpoints/{MODEL_NAME}',\n",
    "          save_best_model=True,\n",
    "          output_path=f'models/{MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054135e1-40a7-475b-9a6b-5d883eb31573",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "\n",
    "After fine-tuning the model, we can evaluate its performance using our test sets.\n",
    "\n",
    "We start by loading the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc906de-f6c3-4535-968e-b75221846772",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(f'models/{MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71832086-928c-4eac-bbad-508452851799",
   "metadata": {},
   "source": [
    "We can then proceed to evaluate its performance on the separate test datasets for LGL, GWN, and TRN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f140735-f2e0-4c78-b4e5-e990e66959b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model([test_lgl_df, test_gwn_df, test_trn_df], ['LGL', 'GWN', 'TRN'], gazetteer_df, model, model_name=MODEL_NAME, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semtor-env",
   "language": "python",
   "name": "semtor-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
